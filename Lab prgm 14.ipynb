{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHt_Ib8iFS16"
      },
      "outputs": [],
      "source": [
        "#This program demonstrates prompt chaining using the GPT-2 model from the Hugging Face transformers library. The model is used to:\n",
        "\n",
        "#Summarize a given sentence.\n",
        "#Answer a question based on that summary.\n",
        "#The chaining process means that the result of one prompt (the summary) is passed as input to the next step (question answering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6QWVkDyEdvk",
        "outputId": "1885804c-ed5e-456e-db86-0545d6cde3b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1 - Generation: Generate the sentence based on the given scenario: This is genAI lab and this is program 3. The method is the same as the previous one, but is a bit different. It is used to assign a sentence to a particular object and the program\n",
            "Step 2 - Answer: Based on this generation: 'Generate the sentence based on the given scenario: This is genAI lab and this is program 3. The method is the same as the previous one, but is a bit different. It is used to assign a sentence to a particular object and the program', answer the question: Which lab is this? The answer is a bit more complicated. You might consider this to be the same as 'generate the sentence based on what the given scenario suggests'. For example, if you want to choose which lab to assign a sentence to, you might use '\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the GPT-2 model for text generation\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Step 1: Generate the sentence based on the given scenario\n",
        "def generate_text(sentence):\n",
        "    prompt = f\"Generate the sentence based on the given scenario: {sentence}\"\n",
        "    text = generator(prompt, max_new_tokens=30, pad_token_id=50256, truncation=True, num_return_sequences=1)[0]['generated_text']\n",
        "    return text\n",
        "\n",
        "# Step 2: Ask a question based on the generated text\n",
        "def ask_question(text, question):\n",
        "    prompt = f\"Based on this generation: '{text}', answer the question: {question}\"\n",
        "    answer = generator(prompt, max_new_tokens=50, pad_token_id=50256, truncation=True, num_return_sequences=1)[0]['generated_text']\n",
        "    return answer\n",
        "\n",
        "# Main function for prompt chaining\n",
        "def prompt_chaining(sentence, question):\n",
        "    # Step 1: Generate the sentence\n",
        "    text = generate_text(sentence)\n",
        "    print(f\"Step 1 - Generation: {text}\")\n",
        "\n",
        "    # Step 2: Ask a question based on the generated text\n",
        "    answer = ask_question(text, question)\n",
        "    print(f\"Step 2 - Answer: {answer}\")\n",
        "\n",
        "# Test the prompt chaining process\n",
        "sentence = \"This is genAI lab and this is program 3.\"\n",
        "question = \"Which lab is this?\"\n",
        "prompt_chaining(sentence, question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM-Pl-ZwG5SO",
        "outputId": "7efb0d22-5977-4e27-ce43-1f7a6b35dc30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1 - Generated Text: Generate a sentence based on this scenario: John went to the market to buy apples and oranges. John goes to the store to buy some produce.\n",
            "\n",
            "Then, John goes to the store and sells his product as he walks to the store.\n",
            "Step 2 - Answer: Based on this sentence: 'Generate a sentence based on this scenario: John went to the market to buy apples and oranges. John goes to the store to buy some produce.\n",
            "\n",
            "Then, John goes to the store and sells his product as he walks to the store.', answer this question: Why john dint buy oranges, oranges, oranges?\n",
            "\n",
            "John would have to buy produce from the store, and then he would have to sell it to the grocer to\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the GPT-2 model for text generation\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Step 1: Generate a sentence based on a scenario\n",
        "def generate_scenario_text(sentence):\n",
        "    prompt = f\"Generate a sentence based on this scenario: {sentence}\"\n",
        "    text = generator(prompt, max_new_tokens=30, pad_token_id=50256, truncation=True, num_return_sequences=1)[0]['generated_text']\n",
        "    return text\n",
        "\n",
        "# Step 2: Ask a question based on the generated text\n",
        "def ask_followup_question(text, question):\n",
        "    prompt = f\"Based on this sentence: '{text}', answer this question: {question}\"\n",
        "    answer = generator(prompt, max_new_tokens=30, pad_token_id=50256, truncation=True, num_return_sequences=1)[0]['generated_text']\n",
        "    return answer\n",
        "\n",
        "# Main function for prompt chaining\n",
        "def prompt_chaining(sentence, question):\n",
        "    # Step 1: Generate the text based on the input scenario\n",
        "    generated_text = generate_scenario_text(sentence)\n",
        "    print(f\"Step 1 - Generated Text: {generated_text}\")\n",
        "\n",
        "    # Step 2: Ask a follow-up question based on the generated text\n",
        "    answer = ask_followup_question(generated_text, question)\n",
        "    print(f\"Step 2 - Answer: {answer}\")\n",
        "\n",
        "# Test the prompt chaining process\n",
        "sentence = \"John went to the market to buy apples and oranges.\"\n",
        "question = \"Why john dint buy\"\n",
        "prompt_chaining(sentence, question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "713yoml3KGZ0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gen-ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
